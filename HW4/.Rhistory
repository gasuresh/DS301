for (i in seq_along(folds)) {
test_index = folds[[i]]
train_data <- Boston[-test_index, ]
test_data <- Boston[test_index, ]
model <- lm(medv ~ poly(lstat, d, raw = TRUE), data = train_data)
valid_pred <- predict(model, newdata = test_data)
fold_errors[i] <- mean((test_data$medv - valid_pred)^2)
}
results$CV_Error[results$Degree == d] <- mean(fold_errors)
}
ggplot(results, aes(x = Degree, y = CV_Error)) +
geom_point() +
geom_line() +
labs(x = "Polynomial Degree", y = "Test MSE",
title = "Test MSE vs. Polynomial Degree")
optimal_degree <- results$Degree[which.min(results$CV_Error)]
degrees <- 1:9
results <- data.frame(Degree = degrees, CV_Error = numeric(length(degrees)))
set.seed(1)
folds <- createFolds(Boston$medv, k = 5)
for (d in degrees) {
fold_errors <- numeric(length(folds))
for (i in seq_along(folds)) {
test_index = folds[[i]]
train_data <- Boston[-test_index, ]
test_data <- Boston[test_index, ]
model <- lm(medv ~ poly(lstat, d, raw = TRUE), data = train_data)
valid_pred <- predict(model, newdata = test_data)
fold_errors[i] <- mean((test_data$medv - valid_pred)^2)
}
results$CV_Error[results$Degree == d] <- mean(fold_errors)
}
ggplot(results, aes(x = Degree, y = CV_Error)) +
geom_point() +
geom_line() +
labs(x = "Polynomial Degree", y = "Cross-Validated Error",
title = "Cross-Validated Error vs. Polynomial Degree")
optimal_degree <- results$Degree[which.min(results$CV_Error)]
degrees <- 1:9
results <- data.frame(Degree = degrees, CV_Error = numeric(length(degrees)))
set.seed(1)
folds <- createFolds(Boston$medv, k = 5)
for (d in degrees) {
fold_errors <- numeric(length(folds))
for (i in seq_along(folds)) {
test_index = folds[[i]]
train_data <- Boston[-test_index, ]
test_data <- Boston[test_index, ]
model <- lm(medv ~ poly(lstat, d, raw = TRUE), data = train_data)
valid_pred <- predict(model, newdata = test_data)
fold_errors[i] <- mean((test_data$medv - valid_pred)^2)
}
results$CV_Error[results$Degree == d] <- mean(fold_errors)
}
ggplot(results, aes(x = Degree, y = CV_Error)) +
geom_point() +
geom_line() +
labs(x = "Polynomial Degree", y = "Cross-Validated Error",
title = "Cross-Validated Error (MSE) vs. Polynomial Degree")
optimal_degree <- results$Degree[which.min(results$CV_Error)]
set.seed(1)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = sqrt(1))
y <- x - 2 * x^2 + error
errors
#4c.
n <- length(x)
errors <- numeric(4)
for (i in 1:4) {
if (i == 1) {
model <- lm(y ~ x)
} else {
model <- lm(y ~ poly(x, i, raw = TRUE))
}
loocv_error <- sum((residuals(model) / (1 - lm.influence(model)$hat))^2) / n
errors[i] <- loocv_error
}
erros
errors
LOOCV_errors <- numeric(4)
# Perform LOOCV for each model
for (i in 1:4) {
# Initialize vector to store squared residuals for LOOCV
squared_residuals <- numeric(length(x))
# Perform Leave-One-Out Cross-Validation
for (j in 1:length(x)) {
# Leave out one observation
x_leave_one_out <- x[-j]
y_leave_one_out <- y[-j]
# Fit the model based on the current degree
if (i == 1) {
model <- lm(y_leave_one_out ~ x_leave_one_out)
} else {
model <- lm(y_leave_one_out ~ poly(x_leave_one_out, degree = i))
}
# Predict on the left-out observation
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
# Compute squared residual
squared_residuals[j] <- (y[j] - y_predicted)^2
}
# Compute the LOOCV error for the current model
LOOCV_errors[i] <- mean(squared_residuals)
}
LOOCV_errors <- numeric(4)
# Perform LOOCV for each model
LOOCV_errors <- numeric(4)
# Perform Leave-One-Out Cross-Validation for each model
for (i in 1:4) {
# Initialize an empty vector to store squared residuals
squared_residuals <- numeric(length(x))
# Perform LOOCV
for (j in 1:length(x)) {
# Fit the model on the data without the j-th observation
if (i == 1) {
model <- lm(y[-j] ~ x[-j])
} else {
model <- lm(y[-j] ~ poly(x[-j], i))
}
# Predict on the left-out observation
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
# Compute squared residual
squared_residuals[j] <- (y[j] - y_predicted)^2
}
# Compute the LOOCV error for the current model
LOOCV_errors[i] <- mean(squared_residuals)
}
#4b.
set.seed(1)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = sqrt(1))
y <- x - 2 * x^2 + error
#4c.
n <- length(x)
errors <- numeric(4)
for (i in 1:4) {
if (i == 1) {
model <- lm(y ~ x)
} else {
model <- lm(y ~ poly(x, i, raw = TRUE))
}
loocv_error <- sum((residuals(model) / (1 - lm.influence(model)$hat))^2) / n
errors[i] <- loocv_error
}
LOOCV_errors <- numeric(4)
# Perform LOOCV for each model
LOOCV_errors <- numeric(4)
# Perform Leave-One-Out Cross-Validation for each model
for (i in 1:4) {
# Initialize an empty vector to store squared residuals
squared_residuals <- numeric(length(x))
# Perform LOOCV
for (j in 1:length(x)) {
# Fit the model on the data without the j-th observation
if (i == 1) {
model <- lm(y[-j] ~ x[-j])
} else {
model <- lm(y[-j] ~ poly(x[-j], i))
}
# Predict on the left-out observation
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
# Compute squared residual
squared_residuals[j] <- (y[j] - y_predicted)^2
}
# Compute the LOOCV error for the current model
LOOCV_errors[i] <- mean(squared_residuals)
}
LOOCV_errors <- numeric(4)
# Perform Leave-One-Out Cross-Validation for each model
for (i in 1:4) {
# Initialize an empty vector to store squared residuals
squared_residuals <- numeric(length(x))
# Perform LOOCV
for (j in 1:length(x)) {
# Fit the model on the data without the j-th observation
if (i == 1) {
model <- lm(y[-j] ~ x[-j])
} else {
model <- lm(y[-j] ~ poly(x[-j], i))
}
# Predict on the left-out observation
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
# Compute squared residual
squared_residuals[j] <- (y[j] - y_predicted)^2
}
# Compute the LOOCV error for the current model
LOOCV_errors[i] <- mean(squared_residuals)
}
LOOCV_errors <- numeric(4)
# Perform Leave-One-Out Cross-Validation for each model
for (i in 1:4) {
# Initialize vector to store squared residuals for LOOCV
squared_residuals <- numeric(n)
# Fit the model for current iteration
if (i == 1) {
model_formula <- as.formula(paste("y ~ x"))
} else {
model_formula <- as.formula(paste("y ~ poly(x, ", i, ")", sep = ""))
}
# Perform Leave-One-Out Cross-Validation
for (j in 1:n) {
# Create training data without j-th observation
x_train <- x[-j]
y_train <- y[-j]
# Fit the model on the training data
model <- lm(model_formula, data = data.frame(x = x_train, y = y_train))
# Predict on the left-out observation
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
# Compute squared residual
squared_residuals[j] <- (y[j] - y_predicted)^2
}
# Compute LOOCV error for the current model
LOOCV_errors[i] <- mean(squared_residuals)
}
LOOCV_errors
errors
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_residuals <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("y ~ x"))
} else {
model_formula <- as.formula(paste("y ~ poly(x, ", i, ")", sep = ""))
}
for (j in 1:n) {
x_train <- x[-j]
y_train <- y[-j]
model <- lm(model_formula, data = data.frame(x = x_train, y = y_train))
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
MSE[j] <- (y[j] - y_predicted)^2
}
LOOCV_errors[i] <- mean(MSE)
}
LOOCV_errors <- numeric(4)
for (i in 1:4) {
MSE <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("y ~ x"))
} else {
model_formula <- as.formula(paste("y ~ poly(x, ", i, ")", sep = ""))
}
for (j in 1:n) {
x_train <- x[-j]
y_train <- y[-j]
model <- lm(model_formula, data = data.frame(x = x_train, y = y_train))
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
MSE[j] <- (y[j] - y_predicted)^2
}
LOOCV_errors[i] <- mean(MSE)
}
LOOCV_errors <- numeric(4)
for (i in 1:4) {
MSE <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("y ~ x"))
} else {
model_formula <- as.formula(paste("y ~ poly(x, ", i, ")", sep = ""))
}
for (j in 1:n) {
x_train <- x[-j]
y_train <- y[-j]
model <- lm(model_formula, data = data.frame(x = x_train, y = y_train))
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
MSE[j] <- (y[j] - y_predicted)^2
}
LOOCV_errors[i] <- mean(MSE)
}
LOOCV_errors <- numeric(4)
for (i in 1:4) {
MSE <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("y ~ x"))
} else {
model_formula <- as.formula(paste("y ~ poly(x, ", i, ")", sep = ""))
}
for (j in 1:n) {
x_train <- x[-j]
y_train <- y[-j]
model <- lm(model_formula, data = data.frame(x = x_train, y = y_train))
# newdata is test data
y_predicted <- predict(model, newdata = data.frame(x = x[j]))
MSE[j] <- (y[j] - y_predicted)^2
}
LOOCV_errors[i] <- mean(MSE)
}
data <- data.frame(X = X, Y = Y)
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
MSE <- numeric(n)
if (i == 1) {
model <- lm(Y ~ X, data = data)
} else {
model <- lm(Y ~ poly(X, i), data = data)
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model$formula, data = data_train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
MSE[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(MSE)
}
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
MSE <- numeric(n)
if (i == 1) {
model <- lm(Y ~ X, data = data)
} else {
model <- lm(Y ~ poly(X, i), data = data)
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model$formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
MSE[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(MSE)
}
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model <- lm(Y ~ X, data = data)
} else {
model <- lm(Y ~ poly(X, i), data = data)
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model$formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
LOOCV_errors
errors
#4d.
set.seed(2)
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
LOOCV_errors
#4b.
set.seed(1)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = sqrt(1))
y <- x - 2 * x^2 + error
#4d.
set.seed(2)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = sqrt(1))
y <- x - 2 * x^2 + error
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
LOOCV_errors
#4c.
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
smallest_error_model <- which.min(LOOCV_errors)
smallest_error_model
#4b.
set.seed(1)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = 1^2)
y <- x - 2 * x^2 + error
#4c.
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
smallest_error_model <- which.min(LOOCV_errors)
#4d.
set.seed(2)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = sqrt(1))
y <- x - 2 * x^2 + error
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
#4d.
set.seed(2)
x <- rnorm(100)
error <- rnorm(100, mean = 0, sd = 1^2)
y <- x - 2 * x^2 + error
data <- data.frame(X = x, Y = y)
n <- nrow(data)
LOOCV_errors <- numeric(4)
for (i in 1:4) {
squared_error <- numeric(n)
if (i == 1) {
model_formula <- as.formula(paste("Y ~ X"))
} else {
model_formula <- as.formula(paste("Y ~ poly(X, ", i, ")", sep = ""))
}
for (j in 1:n) {
train <- data[-j,]
test <- data[j,]
model_leave_one_out <- lm(model_formula, data = train)
Y_predicted <- predict(model_leave_one_out, newdata = test)
squared_error[j] <- (data$Y[j] - Y_predicted)^2
}
LOOCV_errors[i] <- mean(squared_error)
}
smallest_error_model
