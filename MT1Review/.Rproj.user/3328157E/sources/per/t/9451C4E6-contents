---
title: "HW3"
author: "Gautham Suresh"
date: "2024-02-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ISLR2)
library(tidyverse)
```

```{r}
head(Carseats)
```

### Problem #1

```{r}
#1a.
model <- lm(Sales ~ CompPrice + Income + Advertising + Population + Price + Age + Education + Urban + US, data = Carseats)

coefficients_table <- as.data.frame(summary(model)$coefficients[, c("Estimate", "Std. Error")])

rownames(coefficients_table)[rownames(coefficients_table) == "UrbanYes"] <- "Urban"
rownames(coefficients_table)[rownames(coefficients_table) == "USYes"] <- "US"

```

```{r}
#1a.
age_summary <- summary(model)$coefficients["Age", c("t value", "Pr(>|t|)")]
alpha <- 0.05

reject <- age_summary["Pr(>|t|)"] < alpha ## reject if true

n <- dim(Carseats)[1]
p <- 9
df <- n - (p+1)

```

1a. **Hypothesis Testing**

-   *Null Hypothesis*
    -   $H_0: \beta_{Age} = 0$
-   *Alternative Hypothesis*
    -   $H_0: \beta_{Age} \neq 0$
-   *Test Statistic*
    -   $\frac{\hat{\beta}_{Age}}{se(\hat{\beta}_{Age})} = -7.44$
-   *Null Distribution*
    -   Under the null hypothesis, the t-statistic for the variable "Age" follows a t-distribution with degrees of freedom equal to the total number of observations minus the number of predictors in the model
    -   $df = \text{num_observ} - \text{num_pred} = 390$
-   *P-Value*
    -   $p\textrm{-}value = 6.59*10^{-13}$
-   *Conclusion*
    -   With a very small p-value $(6.59*10^{-13} < 0.05)$, we reject the null hypothesis, meaning there is strong evidence that the coefficient for the variable "Age" is not equal to zero

1b. We assumed the errors in the model are normally distributed

```{r}
#1c.
sigma_sqr <- sum(model$residual^2)/(n-(p+1))
```

1c. $\sigma^2 = 3.73$. The value is a measure of the average amount by which the actual data points deviate from the predicted values generated by the regression model

1d. The estimated regression coefficient associated with "Advertising" based on the model is 0.1369.This means For every one-unit (\$1000) increase in the advertising budget, we expect, on average, an increase of 0.1369 units (\$136.90) in the sales of car seats

```{r}
#1e.
residuals_full <- residuals(model)

rss_full <- sum(residuals_full^2)

reduced_model <- lm(Sales~1, data = Carseats)

residuals_reduced <- residuals(reduced_model)

rss_reduced <- sum(residuals_reduced^2)
```

1e. RSS_Full = 1456.03, RSS_reduced = 3182.28

1f. **F-Test**

-   *Null Hypothesis*
    -   $H_0 = \beta_0 = \beta_1 = \ldots = \beta_k = 0$
    -   In this case $H_0 = \beta_{CompPrice} = \beta_{Income} = \ldots = \beta_{US} = 0$ for the predictors of the full model
-   *Alternative Hypothesis*
    -   $H_1$: at least one $\beta_j \neq 0$
-   *Test Statistic*
    -   $F^* = \frac{(RSS_R - RSS_F)/(df_R - df_F)}{RSS_F/df_F}$
    -   Based on the model calculations and the above formula $F^* = 51.38$
-   *Null Distribution*
    -   Under the null hypothesis, the test statistic $F$ follows an F-distribution with $k$ and $n - p - 1$ degrees of freedom
    -   The test statistic $F$ follows an F-distribution with $9$ and $390$ degrees of freedom
-   *P-value*
    -   Based on the model, the p-value is $2.2*10^{-16}$
-   *Conclusion*
    -   Since the p-value is less than $\alpha = 0.05$, we reject the null hypothesis. This means there is evidence of a relationship between $Y$ and at least one of the predictors, at $\alpha = 0.05$

```{r}
#1g.
new_data <- data.frame(
  CompPrice = mean(Carseats$CompPrice),
  Income = median(Carseats$Income),
  Advertising = 15,
  Population = 500,
  Price = 50,
  Age = 30,
  Education = 10,
  Urban = "Yes",
  US = "Yes"
)

confidence_interval <- predict(model, newdata = new_data, interval = "confidence", level = 0.95)
```

1g. **f(X) Estimates**

-   Estimated f(X) = 15.81 units (\$158,100)
-   Confidence Interval
    -   Lower: 14.92 units (\$149,200)
    -   Upper: 16.69 units (\$166,900)

```{r}
#1h.
prediction_interval <- predict(model, newdata = new_data, interval = "prediction", level = 0.95)
```

1h. **Y Estimates**

-   Estimated Y = 15.81 units (\$158,100)
-   Confidence Interval
    -   Lower: 11.91 units (\$119,100)
    -   Upper: 19.71 units (\$197,100)

1i. **Fixed Values of X**

-   $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon$ by definition
-   The expected value of $Y$ given fixed values of $X$ $(X_1, X_1,..., X_p)$ can be written as $E(Y) = E(\beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon)$
-   The equation can be simplified to $E(Y) = E(\beta_0) + E(\beta_1X_1) + E(\beta_2X_2) + \cdots + E(\beta_pX_p) + E(\epsilon))$
-   Given fixed values, we know that $E(X_i) = X_i$
-   We also know $E(\epsilon) = 0$
-   By definition $f(X) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p$
-   Since $E(X_i) = X_i$, $E(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p$
-   Therefore, $f(X) = E(Y)$ and the interval quantifies the uncertainty surrounding our estimate for the expected value $E(Y)$ for a fixed value of $X$

```{r}
#1j.
new_data$Price <- 450

new_confidence_interval <- predict(model, newdata = new_data, interval = "confidence", level = 0.95)
```

1j. The prediction for $Y$ is -21.17. This value does not make sense as we are predicting the number of sales. Linear regression assumes a linear relationship between predictors and the response. If the true relationship is not linear, it may lead to inaccurate predictions, especially seeing as we are extrapolating beyond the observed range of the training data by setting the new price to be 450, considering the maximum value in the training data is 191.

1k. The unbiasedness property of an estimator means that, on average, the estimator's expected value is equal to the true parameter it is estimating. In this case $Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p + \epsilon$ and $\hat{Y} = \hat{\beta_0} + \hat{\beta_1}X_1 + \hat{\beta_2}X_2 + \cdots + \hat{\beta_p}X_p$. We know that the assumptions needed to fit a least squares regression model hold and that multiple linear regression assumes that the errors $\epsilon$ have a mean of zero. Therefore, $E(\hat{Y}) = E(\hat{\beta_0} + \hat{\beta_1}X_1 + \hat{\beta_2}X_2 + \cdots + \hat{\beta_p}X_p) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \cdots + \beta_pX_p$, $E(\hat{Y}) = Y$

### Problem #2

2a. If we want to test $m$ null hypothesis and we simply reject all null hypothesis for which the corresponding p-value falls below $\alpha$, we can expect to make $m*\alpha$ type 1 errors

```{r}
#2b.
results_df <- data.frame(num_tests = numeric(), false_positives = numeric())

for (p in seq(150, 800, by = 50)) {
  x <- matrix(rnorm(1000 * p), ncol = p)
  
  beta <- c(2, 3, 5, 4, 2, rep(0, p - 5))
  error <- rnorm(1000, 0, 1)
  y <- as.matrix(x) %*% beta + error
  
  data <- as.data.frame(cbind(y, x))
  
  fit <- lm(y ~ ., data = data)
  
  p_values <- suppressWarnings(coef(summary(fit))[, "Pr(>|t|)"])
  
  num_false_positives <- sum(p_values < 0.05)
  
  results_df <- rbind(results_df, data.frame(num_tests = p, false_positives = num_false_positives))
}

ggplot(results_df, aes(x = num_tests, y = false_positives)) +
  geom_line() +
  geom_point() +
  labs(title = "Impact of Multiple Testing on False Positives",
       x = "Num Tests",
       y = "Num of False Positives")


```

2b. The number of false positives increase as the number of tests increase

### Problem #3

3a. The true values are $\beta_0 = 2$, $\beta_1 = 3$, $\beta_2 = 5$

```{r}
#3b.
set.seed(1)

n <- 100
num_simulations <- 5000

X1 <- seq(0, 10, length.out = n)
X2 <- runif(n)

beta0 <- 2
beta1 <- 3
beta2 <- 5

sigma <- 2

error <- rnorm(n, mean = 0, sd = sigma)

Y <- beta0 + beta1 * X1 + beta2 * X2 + error

```

```{r}
#3c.
sim_results <- numeric(num_simulations)

for (i in 1:num_simulations) {
  error <- rnorm(n, mean = 0, sd = sigma)
  Y <- beta0 + beta1 * X1 + beta2 * X2 + error
  
  sim_model <- lm(Y ~ X1 + X2)
  
  residuals <- sim_model$residuals
  pred_sigma_squared <- sum(residuals^2) / (n - 3)
  sim_results[i] <- pred_sigma_squared
}

estimated_sigma_sqr <- mean(sim_results)

```

3c. By running this simulation for 5000 iterations, we see that the predicted $\hat{\sigma}^2 = 3.9993$ compared to the true $\sigma^2 = 4$. Since the predicted value approaches the true value based on the average, it is an unbiased estimator

```{r}
#3d.
df <- data.frame(pred_sigma_squared = sim_results)

ggplot(df, aes(x = pred_sigma_squared)) +
  geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black", alpha = 0.7) +
  geom_vline(xintercept = sigma^2, color = "red", linetype = "dashed") +
  labs(title = "Distribution of Predicted sigma^2",
       x = "Predicted sigma^2",
       y = "Frequency") +
  theme_minimal()
```

3e. The estimate of $\sigma^2$ is important as it relates to the standard error of the model. Standard error quantifies the variance between $\hat{\beta}_i$ and $\beta_i$. If we do not have a good estimate of $\hat{\sigma}^2$, it affects hypothesis testing as t-statistic values are dependent on the standard error values, which are dependent on $\hat{\sigma}^2$. The construction of prediction intervals and confidence intervals also depends on $\hat{\sigma}^2$

### Problem #4

4a. Setting the significance level $(\alpha)$ to 0.05 in hypothesis testing means you are establishing a threshold for deciding whether a particular variable has a significant impact on your model or if the observed relationship could be due to random chance. We're willing to accept a 5% chance that any observed relationship between variables in our model is due to random chance. If the p-value is below 0.05, we conclude the relationship is significant; otherwise, we don't.

4b. When the p-value is greater than the significance level, we do not reject the null hypothesis. This suggests that there is evidence to say that the predictor $X_j$ has a significant impact on the response variable as $\beta_j$ is different from 0. In this case we would disagree with the collaborator's claim

4c. **Future plans**

-   When you perform multiple hypothesis tests simultaneously, there is an increased chance of obtaining at least one statistically significant result purely by chance, even if all the null hypotheses are true. This is because, with each test, there is some probability of making a Type I error (rejecting a true null hypothesis)
-   In this case, the scientist plans to conduct 12 individual t-tests, and if any one of them yields a significant result, they would conclude that at least one predictor is useful. The more tests you do, the higher the chance of obtaining a significant result purely due to random variation, even if none of the predictors is truly useful
-   For 12 independent tests, the probability of not making a Type 1 error in any of them is $(1 - \alpha)^{12}$. Then the probability of seeing at least one significant result is $1 - (1 - \alpha)^{12} = 1 - (0.9)^{12} = .72$
