set.seed(1)
n <- 100
num_simulations <- 1000
X <- seq(0, 5, length.out = n)
beta0 <- 4
beta1 <- 1
beta2 <- 1
sigma <- 1
error <- rnorm(n, mean = 0, sd = sigma)
Y <- beta0 + beta1 * X + beta2 * X^2 + error
train_set = data.frame(X,Y)
M1 <- lm(Y~X, data = train_set)
M2 <- lm(Y~poly(x, degree=2), data = train_set)
set.seed(1)
n <- 100
num_simulations <- 1000
X <- seq(0, 5, length.out = n)
beta0 <- 4
beta1 <- 1
beta2 <- 1
sigma <- 1
error <- rnorm(n, mean = 0, sd = sigma)
Y <- beta0 + beta1 * X + beta2 * X^2 + error
train_set = data.frame(X,Y)
M1 <- lm(Y~X, data = train_set)
M2 <- lm(Y~poly(x, degree=2), data = train_set)
set.seed(1)
n <- 100
num_simulations <- 1000
X <- seq(0, 5, length.out = n)
beta0 <- 4
beta1 <- 1
beta2 <- 1
sigma <- 1
error <- rnorm(n, mean = 0, sd = sigma)
Y <- beta0 + beta1 * X + beta2 * X^2 + error
train_set = data.frame(X,Y)
M1 <- lm(Y~X, data = train_set)
M2 <- lm(Y~poly(x, degree=2), data = train_set)
mean((y0 - yhat1)^2)
set.seed(1)
n <- 100
num_simulations <- 1000
x <- seq(0, 5, length.out = n)
beta0 <- 4
beta1 <- 1
beta2 <- 1
sigma <- 1
error <- rnorm(n, mean = 0, sd = sigma)
y <- beta0 + beta1 * x + beta2 * x^2 + error
train_set = data.frame(X,Y)
M1 <- lm(y~x, data = train_set)
M2 <- lm(y~poly(x, degree=2), data = train_set)
M3 <- lm(y~poly(x, degree=3), data = train_set)
M4 <- lm(y~poly(x, degree=5), data = train_set)
M5 <- lm(y~poly(x, degree=11), data = train_set)
yhat1 = yhat2 = yhat3 = yhat4 = yhat5 = rep(NA, 1000)
for (i in 1:1000)
{
error = rnorm(n,0,1)
y <- beta0 + beta1 * x + beta2 * x^2 + error
train_set = data.frame(x,y)
M1 <- lm(y~x, data = train_set)
M2 <- lm(y~poly(x, degree=2), data = train_set)
M3 <- lm(y~poly(x, degree=3), data = train_set)
M4 <- lm(y~poly(x, degree=5), data = train_set)
M5 <- lm(y~poly(x, degree=11), data = train_set)
yhat1[i] = predict(M1, data.frame(x=1))
yhat2[i] = predict(M2, data.frame(x=1))
yhat3[i] = predict(M3, data.frame(x=1))
yhat4[i] = predict(M4, data.frame(x=1))
yhat5[i] = predict(M5, data.frame(x=1))
}
x0 = rep(1, 1000)
y0 = 1 + x0 + x0^2 + rnorm(1000, 0, 1)
test = data.frame(x0, y0)
mean((y0 - yhat1)^2)
mean((y0 - yhat2)^2)
mean((y0 - yhat3)^2)
mean((y0 - yhat4)^2)
mean((y0 - yhat5)^2)
set.seed(1)
n <- 100
num_simulations <- 1000
x <- seq(0, 5, length.out = n)
beta0 <- 4
beta1 <- 1
beta2 <- 1
sigma <- 1
error <- rnorm(n, mean = 0, sd = sigma)
y <- beta0 + beta1 * x + beta2 * x^2 + error
train_set = data.frame(X,Y)
M1 <- lm(y~x, data = train_set)
M2 <- lm(y~poly(x, degree=2), data = train_set)
M3 <- lm(y~poly(x, degree=3), data = train_set)
M4 <- lm(y~poly(x, degree=5), data = train_set)
M5 <- lm(y~poly(x, degree=11), data = train_set)
yhat1 = yhat2 = yhat3 = yhat4 = yhat5 = rep(NA, 1000)
for (i in 1:1000)
{
error = rnorm(n,0,1)
y <- beta0 + beta1 * x + beta2 * x^2 + error
train_set = data.frame(x,y)
M1 <- lm(y~x, data = train_set)
M2 <- lm(y~poly(x, degree=2), data = train_set)
M3 <- lm(y~poly(x, degree=3), data = train_set)
M4 <- lm(y~poly(x, degree=5), data = train_set)
M5 <- lm(y~poly(x, degree=11), data = train_set)
yhat1[i] = predict(M1, data.frame(x=1))
yhat2[i] = predict(M2, data.frame(x=1))
yhat3[i] = predict(M3, data.frame(x=1))
yhat4[i] = predict(M4, data.frame(x=1))
yhat5[i] = predict(M5, data.frame(x=1))
}
x0 = rep(1, 1000)
y0 = 1 + x0 + x0^2 + rnorm(1000, 0, 1)
test = data.frame(x0, y0)
mean((y0 - yhat1)^2)
mean((y0 - yhat2)^2)
mean((y0 - yhat3)^2)
mean((y0 - yhat4)^2)
mean((y0 - yhat5)^2)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
library(tidyverse)
head(Carseats)
#1a.
model <- lm(Sales ~ CompPrice + Income + Advertising + Population + Price + Age + Education + Urban + US, data = Carseats)
coefficients_table <- as.data.frame(summary(model)$coefficients[, c("Estimate", "Std. Error")])
rownames(coefficients_table)[rownames(coefficients_table) == "UrbanYes"] <- "Urban"
rownames(coefficients_table)[rownames(coefficients_table) == "USYes"] <- "US"
#1a.
age_summary <- summary(model)$coefficients["Age", c("t value", "Pr(>|t|)")]
alpha <- 0.05
reject <- age_summary["Pr(>|t|)"] < alpha ## reject if true
n <- dim(Carseats)[1]
p <- 9
df <- n - (p+1)
#1c.
sigma_sqr <- sum(model$residual^2)/(n-(p+1))
#1e.
residuals_full <- residuals(model)
rss_full <- sum(residuals_full^2)
reduced_model <- lm(Sales~1, data = Carseats)
residuals_reduced <- residuals(reduced_model)
rss_reduced <- sum(residuals_reduced^2)
#1g.
new_data <- data.frame(
CompPrice = mean(Carseats$CompPrice),
Income = median(Carseats$Income),
Advertising = 15,
Population = 500,
Price = 50,
Age = 30,
Education = 10,
Urban = "Yes",
US = "Yes"
)
confidence_interval <- predict(model, newdata = new_data, interval = "confidence", level = 0.95)
#1h.
prediction_interval <- predict(model, newdata = new_data, interval = "prediction", level = 0.95)
#1j.
new_data$Price <- 450
new_confidence_interval <- predict(model, newdata = new_data, interval = "confidence", level = 0.95)
#2b.
results_df <- data.frame(num_tests = numeric(), false_positives = numeric())
for (p in seq(150, 800, by = 50)) {
x <- matrix(rnorm(1000 * p), ncol = p)
beta <- c(2, 3, 5, 4, 2, rep(0, p - 5))
error <- rnorm(1000, 0, 1)
y <- as.matrix(x) %*% beta + error
data <- as.data.frame(cbind(y, x))
fit <- lm(y ~ ., data = data)
p_values <- suppressWarnings(coef(summary(fit))[, "Pr(>|t|)"])
num_false_positives <- sum(p_values < 0.05)
results_df <- rbind(results_df, data.frame(num_tests = p, false_positives = num_false_positives))
}
ggplot(results_df, aes(x = num_tests, y = false_positives)) +
geom_line() +
geom_point() +
labs(title = "Impact of Multiple Testing on False Positives",
x = "Num Tests",
y = "Num of False Positives")
#3b.
set.seed(1)
n <- 100
num_simulations <- 5000
X1 <- seq(0, 10, length.out = n)
X2 <- runif(n)
beta0 <- 2
beta1 <- 3
beta2 <- 5
sigma <- 2
error <- rnorm(n, mean = 0, sd = sigma)
Y <- beta0 + beta1 * X1 + beta2 * X2 + error
#3c.
sim_results <- numeric(num_simulations)
for (i in 1:num_simulations) {
error <- rnorm(n, mean = 0, sd = sigma)
Y <- beta0 + beta1 * X1 + beta2 * X2 + error
sim_model <- lm(Y ~ X1 + X2)
residuals <- sim_model$residuals
pred_sigma_squared <- sum(residuals^2) / (n - 3)
sim_results[i] <- pred_sigma_squared
}
estimated_sigma_sqr <- mean(sim_results)
#3d.
df <- data.frame(pred_sigma_squared = sim_results)
ggplot(df, aes(x = pred_sigma_squared)) +
geom_histogram(binwidth = 0.1, fill = "lightblue", color = "black", alpha = 0.7) +
geom_vline(xintercept = sigma^2, color = "red", linetype = "dashed") +
labs(title = "Distribution of Predicted sigma^2",
x = "Predicted sigma^2",
y = "Frequency") +
theme_minimal()
age_summary
y(.9)
knitr::opts_chunk$set(echo = TRUE)
set.seed(1) # so we all get the same x values.
n = 100
x = runif(n, min = 0, max = 2)
error = rnorm(n,0,1)
x = 0.9
y = 4 + x + x^2 + x^3
train_set = data.frame(x,y)
M1 = lm(y~x,data=train_set)
M2 = lm(y~poly(x,degree=2),data=train_set)
y
set.seed(1) # so we all get the same x values.
n = 100
x = runif(n, min = 0, max = 2)
error = rnorm(n,0,1)
y = 4 + x + x^2 + x^3 + error
train_set = data.frame(x,y)
M1 = lm(y~x,data=train_set)
M2 = lm(y~poly(x,degree=2),data=train_set)
M3 = lm(y~poly(x,degree=3),data=train_set)
M4 = lm(y~poly(x,degree=5),data=train_set)
M5 = lm(y~poly(x,degree=11),data=train_set)
set.seed(1)
num_simulations <- 1000
x <- seq(0, 5, length.out = n)
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n = 100
x = runif(n, min = 0, max = 2)
error = rnorm(n,0,1)
y = 4 + x + x^2 + x^3 + error
train_set = data.frame(x,y)
M1 = lm(y~x,data=train_set)
M2 = lm(y~poly(x,degree=2),data=train_set)
M3 = lm(y~poly(x,degree=3),data=train_set)
M4 = lm(y~poly(x,degree=5),data=train_set)
M5 = lm(y~poly(x,degree=11),data=train_set)
yhat1 = yhat2 = yhat3 = yhat4 = yhat5 = rep(NA, 1000)
for (i in 1:1000)
{
error = rnorm(n,0,1)
y = 4 + x + x^2 + x^3 + error
train_set = data.frame(x,y)
M1 <- lm(y~x, data = train_set)
M2 <- lm(y~poly(x, degree=2), data = train_set)
M3 <- lm(y~poly(x, degree=3), data = train_set)
M4 <- lm(y~poly(x, degree=5), data = train_set)
M5 <- lm(y~poly(x, degree=11), data = train_set)
yhat1[i] = predict(M1, data.frame(x=1))
yhat2[i] = predict(M2, data.frame(x=1))
yhat3[i] = predict(M3, data.frame(x=1))
yhat4[i] = predict(M4, data.frame(x=1))
yhat5[i] = predict(M5, data.frame(x=1))
}
x0 = rep(1, 1000)
y0 = 4 + x0 + x0^2 + x0^3 + rnorm(1000, 0, 1)
test = data.frame(x0, y0)
mean((y0 - yhat1)^2)
mean((y0 - yhat2)^2)
mean((y0 - yhat3)^2)
mean((y0 - yhat4)^2)
mean((y0 - yhat5)^2)
set.seed(1)
num_simulations <- 1000
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n <- 100
x <- runif(n, min = 0, max = 2)
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
predicted_values <- matrix(NA, nrow = num_simulations, ncol = 5)
for (i in 1:num_simulations) {
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
train_set <- data.frame(x, y)
M1 <- lm(y ~ x, data = train_set)
M2 <- lm(y ~ poly(x, degree = 2), data = train_set)
M3 <- lm(y ~ poly(x, degree = 3), data = train_set)
M4 <- lm(y ~ poly(x, degree = 5), data = train_set)
M5 <- lm(y ~ poly(x, degree = 11), data = train_set)
predicted_values[i, 1] <- predict(M1, newdata = data.frame(x = 0.9))
predicted_values[i, 2] <- predict(M2, newdata = data.frame(x = 0.9))
predicted_values[i, 3] <- predict(M3, newdata = data.frame(x = 0.9))
predicted_values[i, 4] <- predict(M4, newdata = data.frame(x = 0.9))
predicted_values[i, 5] <- predict(M5, newdata = data.frame(x = 0.9))
}
head(predicted_values)
set.seed(1)
num_simulations <- 1000
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n <- 100
x <- runif(n, min = 0, max = 2)
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
# Initialize matrices to store predicted values
yhat1 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat2 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat3 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat4 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat5 <- matrix(NA, nrow = num_simulations, ncol = 5)
for (i in 1:num_simulations) {
# Simulate new Y values
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
train_set <- data.frame(x, y)
# Fit models
M1 <- lm(y ~ x, data = train_set)
M2 <- lm(y ~ poly(x, degree = 2), data = train_set)
M3 <- lm(y ~ poly(x, degree = 3), data = train_set)
M4 <- lm(y ~ poly(x, degree = 5), data = train_set)
M5 <- lm(y ~ poly(x, degree = 11), data = train_set)
# Predict for x = 0.9
x0 <- 0.9
yhat1[i, ] <- predict(M1, newdata = data.frame(x = x0))
yhat2[i, ] <- predict(M2, newdata = data.frame(x = x0))
yhat3[i, ] <- predict(M3, newdata = data.frame(x = x0))
yhat4[i, ] <- predict(M4, newdata = data.frame(x = x0))
yhat5[i, ] <- predict(M5, newdata = data.frame(x = x0))
}
set.seed(1)
num_simulations <- 1000
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n <- 100
x <- runif(n, min = 0, max = 2)
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
# Initialize matrices to store predicted values
yhat1 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat2 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat3 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat4 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat5 <- matrix(NA, nrow = num_simulations, ncol = 5)
for (i in 1:num_simulations) {
# Simulate new Y values
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
train_set <- data.frame(x, y)
# Fit models
M1 <- lm(y ~ x, data = train_set)
M2 <- lm(y ~ poly(x, degree = 2), data = train_set)
M3 <- lm(y ~ poly(x, degree = 3), data = train_set)
M4 <- lm(y ~ poly(x, degree = 5), data = train_set)
M5 <- lm(y ~ poly(x, degree = 11), data = train_set)
# Predict for x = 0.9
x0 <- 0.9
yhat1[i, ] <- predict(M1, newdata = data.frame(x = x0))
yhat2[i, ] <- predict(M2, newdata = data.frame(x = x0))
yhat3[i, ] <- predict(M3, newdata = data.frame(x = x0))
yhat4[i, ] <- predict(M4, newdata = data.frame(x = x0))
yhat5[i, ] <- predict(M5, newdata = data.frame(x = x0))
}
print("Predicted values for model 1:")
print(head(yhat1))
print("Predicted values for model 2:")
print(head(yhat2))
print("Predicted values for model 3:")
print(head(yhat3))
print("Predicted values for model 4:")
print(head(yhat4))
print("Predicted values for model 5:")
print(head(yhat5))
set.seed(1)
num_simulations <- 1000
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n <- 100
x <- runif(n, min = 0, max = 2)
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
# Initialize matrices to store predicted values
yhat1 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat2 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat3 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat4 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat5 <- matrix(NA, nrow = num_simulations, ncol = 5)
for (i in 1:num_simulations) {
# Simulate new Y values
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
train_set <- data.frame(x, y)
# Fit models
M1 <- lm(y ~ x, data = train_set)
M2 <- lm(y ~ poly(x, degree = 2), data = train_set)
M3 <- lm(y ~ poly(x, degree = 3), data = train_set)
M4 <- lm(y ~ poly(x, degree = 5), data = train_set)
M5 <- lm(y ~ poly(x, degree = 11), data = train_set)
# Predict for x = 0.9
x0 <- 0.9
yhat1[i, ] <- predict(M1, newdata = data.frame(x = x0))
yhat2[i, ] <- predict(M2, newdata = data.frame(x = x0))
yhat3[i, ] <- predict(M3, newdata = data.frame(x = x0))
yhat4[i, ] <- predict(M4, newdata = data.frame(x = x0))
yhat5[i, ] <- predict(M5, newdata = data.frame(x = x0))
}
print(predicted_values[1:5, ])
true_value <- 4 + 0.9 + 0.9^2 + 0.9^3
squared_bias <- apply(predicted_values, 2, function(x) mean((mean(x) - true_value)^2))
plot(1:5, squared_bias, type = "b", xlab = "Model Complexity", ylab = "Squared Bias",
main = "Squared Bias vs. Model Complexity")
set.seed(1)
num_simulations <- 1000
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n <- 100
x <- runif(n, min = 0, max = 2)
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
# Initialize matrices to store predicted values
yhat1 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat2 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat3 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat4 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat5 <- matrix(NA, nrow = num_simulations, ncol = 5)
for (i in 1:num_simulations) {
# Simulate new Y values
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
train_set <- data.frame(x, y)
# Fit models
M1 <- lm(y ~ x, data = train_set)
M2 <- lm(y ~ poly(x, degree = 2), data = train_set)
M3 <- lm(y ~ poly(x, degree = 3), data = train_set)
M4 <- lm(y ~ poly(x, degree = 5), data = train_set)
M5 <- lm(y ~ poly(x, degree = 11), data = train_set)
# Predict for x = 0.9
x0 <- 0.9
yhat1[i, ] <- predict(M1, newdata = data.frame(x = x0))
yhat2[i, ] <- predict(M2, newdata = data.frame(x = x0))
yhat3[i, ] <- predict(M3, newdata = data.frame(x = x0))
yhat4[i, ] <- predict(M4, newdata = data.frame(x = x0))
yhat5[i, ] <- predict(M5, newdata = data.frame(x = x0))
}
print(predicted_values[1:5, ])
true_value <- 4 + 0.9 + 0.9^2 + 0.9^3
squared_bias <- apply(predicted_values, 2, function(x) mean((mean(x) - true_value)^2))
plot(1:5, squared_bias, type = "b", xlab = "Model Complexity", ylab = "Squared Bias",
main = "Squared Bias vs. Model Complexity")
true_value <- 4 + 0.9 + 0.9^2 + 0.9^3
squared_bias <- apply(predicted_values, 2, function(x) mean((mean(x) - true_value)^2))
plot(1:5, squared_bias, type = "b", xlab = "Model Complexity", ylab = "Squared Bias",
main = "Squared Bias vs. Model Complexity")
smallest_bias_model <- which.min(squared_bias)
smallest_bias_model <- which.min(squared_bias)
print(smallest_bias_model)
#3.
set.seed(1)
num_simulations <- 1000
beta0 <- 4
beta1 <- 1
beta2 <- 1
beta3 <- 1
n <- 100
x <- runif(n, min = 0, max = 2)
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
# Initialize matrices to store predicted values
yhat1 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat2 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat3 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat4 <- matrix(NA, nrow = num_simulations, ncol = 5)
yhat5 <- matrix(NA, nrow = num_simulations, ncol = 5)
for (i in 1:num_simulations) {
# Simulate new Y values
error <- rnorm(n, 0, 1)
y <- 4 + x + x^2 + x^3 + error
train_set <- data.frame(x, y)
# Fit models
M1 <- lm(y ~ x, data = train_set)
M2 <- lm(y ~ poly(x, degree = 2), data = train_set)
M3 <- lm(y ~ poly(x, degree = 3), data = train_set)
M4 <- lm(y ~ poly(x, degree = 5), data = train_set)
M5 <- lm(y ~ poly(x, degree = 11), data = train_set)
# Predict for x = 0.9
x0 <- 0.9
yhat1[i, ] <- predict(M1, newdata = data.frame(x = x0))
yhat2[i, ] <- predict(M2, newdata = data.frame(x = x0))
yhat3[i, ] <- predict(M3, newdata = data.frame(x = x0))
yhat4[i, ] <- predict(M4, newdata = data.frame(x = x0))
yhat5[i, ] <- predict(M5, newdata = data.frame(x = x0))
}
print(predicted_values[1:5, ])
