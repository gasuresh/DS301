mlr_model = lm(satisf~.,data=patient)
source("C:/Users/gasur/ISU/DS301/MLRNotes/IntroMLR.R")
source("C:/Users/gasur/ISU/DS301/MLRNotes/IntroMLR.R")
#### Multiple Linear Regression ####
# read in data using table read.table()
# make sure you specify the pathway where you saved the data set
patient = read.table("patient.txt",header=FALSE)
head(patient) #look at the first few rows of the data, make sure it's been loaded into R correctly
# give each column its variable name
names(patient) = c("satisf","age","severe","anxiety")
head(patient)
str(patient)
pairs(patient) #pairwise scatterplot
# linear regression model
# use the lm function
model=lm(satisf~age+severe+anxiety,data=patient)
#shortcut:
lm(satisf~.,data=patient)
### What are our least square coefficients?
### What is our fitted model?
summary(model)
names(model)
model$coefficients
model$residuals
model$fitted.values
#use this model to predict Y for specific values of X
x = data.frame(age=40,severe=40,anxiety=1.5)
predict(model,newdata=x)
########################
## In-class Activity  ##
########################
# 1. Fit a linear regression model with the response as patient satisfaction
# and use all other variables as predictors. Report the least square regression coefficients.
# 2. Based on this model, what are the predicted patient satisfaction scores
# for observations 1, 3, and 20?
# 3. Based on this model, what is the predicted patient satisfication score for a patient of age 50, disease severity
# of 27 and anxiety of 3?
# 4. Propose a way to quantify how well our model is able to predict patient
# satisfaction (Y).
## Work in groups to come up with a solution.
## Copy and paste any relevant code on Ed Discussion.
## Please be sure to list all your group members names. Only one group member needs to post on Ed Discussion.
## link: https://edstem.org/us/courses/50620/discussion/
mlr_model = lm(satisf~.,data=patient)
print(mlr_model$coefficients)
#### Multiple Linear Regression ####
# read in data using table read.table()
# make sure you specify the pathway where you saved the data set
patient = read.table("patient.txt",header=FALSE)
head(patient) #look at the first few rows of the data, make sure it's been loaded into R correctly
# give each column its variable name
names(patient) = c("satisf","age","severe","anxiety")
head(patient)
str(patient)
pairs(patient) #pairwise scatterplot
# linear regression model
# use the lm function
model=lm(satisf~age+severe+anxiety,data=patient)
#shortcut:
lm(satisf~.,data=patient)
### What are our least square coefficients?
### What is our fitted model?
summary(model)
names(model)
model$coefficients
model$residuals
model$fitted.values
#use this model to predict Y for specific values of X
x = data.frame(age=40,severe=40,anxiety=1.5)
predict(model,newdata=x)
########################
## In-class Activity  ##
########################
# 1. Fit a linear regression model with the response as patient satisfaction
# and use all other variables as predictors. Report the least square regression coefficients.
# 2. Based on this model, what are the predicted patient satisfaction scores
# for observations 1, 3, and 20?
# 3. Based on this model, what is the predicted patient satisfication score for a patient of age 50, disease severity
# of 27 and anxiety of 3?
# 4. Propose a way to quantify how well our model is able to predict patient
# satisfaction (Y).
## Work in groups to come up with a solution.
## Copy and paste any relevant code on Ed Discussion.
## Please be sure to list all your group members names. Only one group member needs to post on Ed Discussion.
## link: https://edstem.org/us/courses/50620/discussion/
mlr_model = lm(satisf~.,data=patient)
print(mlr_model$coefficients)
temp = data.frame(age=1,severe=3,anxiety=20)
predict(model,newdata=temp)
View(patient)
predict(model)
predict(model)
#### Multiple Linear Regression ####
# read in data using table read.table()
# make sure you specify the pathway where you saved the data set
patient = read.table("patient.txt",header=FALSE)
head(patient) #look at the first few rows of the data, make sure it's been loaded into R correctly
# give each column its variable name
names(patient) = c("satisf","age","severe","anxiety")
head(patient)
str(patient)
pairs(patient) #pairwise scatterplot
# linear regression model
# use the lm function
model=lm(satisf~age+severe+anxiety,data=patient)
#shortcut:
lm(satisf~.,data=patient)
### What are our least square coefficients?
### What is our fitted model?
summary(model)
names(model)
model$coefficients
model$residuals
model$fitted.values
#use this model to predict Y for specific values of X
x = data.frame(age=40,severe=40,anxiety=1.5)
predict(model,newdata=x)
########################
## In-class Activity  ##
########################
# 1. Fit a linear regression model with the response as patient satisfaction
# and use all other variables as predictors. Report the least square regression coefficients.
# 2. Based on this model, what are the predicted patient satisfaction scores
# for observations 1, 3, and 20?
# 3. Based on this model, what is the predicted patient satisfication score for a patient of age 50, disease severity
# of 27 and anxiety of 3?
# 4. Propose a way to quantify how well our model is able to predict patient
# satisfaction (Y).
## Work in groups to come up with a solution.
## Copy and paste any relevant code on Ed Discussion.
## Please be sure to list all your group members names. Only one group member needs to post on Ed Discussion.
## link: https://edstem.org/us/courses/50620/discussion/
mlr_model = lm(satisf~.,data=patient)
print(mlr_model$coefficients)
predict(model)
temp = data.frame(age=50,severe=27,anxiety=3)
predict(model, newdata = temp)
print(mlr_model)
print(mlr_model)
n = 100
beta_0 = 7
beta_1 = 12
X1 = seq(0, 10, length.out = n)
B = 5000
b0 = b1 = rep(NA, B)
for (i in 1:B)
{
error = rnorm(n, 0, 1)
Y = beta_0 + beta_1*X1 + error
m1 = lm(Y - X1)
b0[i] = m1$coefficients[[1]]
b1[i] = m1$coefficients[[2]]
}
error = rnorm(n, 0, 1)
source("C:/Users/gasur/ISU/DS301/MLRNotes/leastsq_properties.R")
source("C:/Users/gasur/ISU/DS301/MLRNotes/leastsq_properties.R")
b1
#########################################################
######### Accuracy of our prediction? ###################
patient = read.table("patient.txt",header=FALSE)
names(patient) = c("satisf","age","severe","anxiety")
## divide our data into a training set and a test set
n = dim(patient)[1]
set.seed(100)
train_index = sample(1:n,n/2,rep=FALSE)
train_patients = patient[train_index,]
test_patients = patient[-train_index,]
model_train = lm(satisf~age+severe+anxiety,data=train_patients)
MSE_train = mean((train_patients$satisf - model_train$fitted.values)^2)
MSE_train
predicted_values = predict(model_train,test_patients)
MSE_test = mean((test_patients$satisf - predicted_values)^2)
MSE_test
# will the training MSE always be smaller than the test MSE?
#########################################################
####### Properties of our linear regression model #######
#########################################################
## let's simulate data where we know the true population
## regression line
## Simple linear regression
## suppose we have 1 single predictor (X1)
n = 100
beta_0 =
beta_1 =
X1 = seq(0,10, length.out=n)
## Generate (or sample) Y based on true population regression
## line
error = rnorm(n,0,1) ## generate 100 error values from
## normal distribution with mean 0 and sd 1.
Y = beta_0 + beta_1*X1 + error
length(error)
length(Y)
## Then we use this sample to estimate the least square line.
## If we were to take a different sample of Y, we would get a different least square line
## and different estimates for beta0 and beta1. Re-run lines 42 - 53 many times to see that the
## estimate coefficients would change.
lm(Y~X1)
## We hope that over many many many samples, on average, our estimates
## would exactly equal the truth.
## This is the idea of an unbiased estimator.
## How can we check this using simulations?
########################
## In-class Activity  ##
########################
## 1. Design a simulation to check whether or not our least square estimates for beta_0 and beta_1 are unbiased.
## 2. After finishing HW 1, explain why using many simple linear regression models is not sufficient
## compared to a multiple linear regression model.
## Note that fitting a simple linear regression model is computationally
## instantaneous since there are analytical solutions for our least square estimates.
## Work in groups to come up with a solution.
## Copy and paste any relevant code on Ed Discussion.
## Please be sure to list all your group members names. Only one group member needs to post on Ed Discussion.
## link: https://edstem.org/us/courses/50620/discussion/
n = 100
beta_0 = 7
beta_1 = 12
X1 = seq(0, 10, length.out = n)
B = 5000
b0 = b1 = rep(NA, B)
for (i in 1:B)
{
error = rnorm(n, 0, 1)
Y = beta_0 + beta_1*X1 + error
m1 = lm(Y - X1)
b0[i] = m1$coefficients[[1]]
b1[i] = m1$coefficients[[2]]
}
#########################################################
######### Accuracy of our prediction? ###################
patient = read.table("patient.txt",header=FALSE)
names(patient) = c("satisf","age","severe","anxiety")
## divide our data into a training set and a test set
n = dim(patient)[1]
set.seed(100)
train_index = sample(1:n,n/2,rep=FALSE)
train_patients = patient[train_index,]
test_patients = patient[-train_index,]
model_train = lm(satisf~age+severe+anxiety,data=train_patients)
MSE_train = mean((train_patients$satisf - model_train$fitted.values)^2)
MSE_train
predicted_values = predict(model_train,test_patients)
MSE_test = mean((test_patients$satisf - predicted_values)^2)
MSE_test
# will the training MSE always be smaller than the test MSE?
#########################################################
####### Properties of our linear regression model #######
#########################################################
## let's simulate data where we know the true population
## regression line
## Simple linear regression
## suppose we have 1 single predictor (X1)
n = 100
beta_0 =
beta_1 =
X1 = seq(0,10, length.out=n)
## Generate (or sample) Y based on true population regression
## line
error = rnorm(n,0,1) ## generate 100 error values from
## normal distribution with mean 0 and sd 1.
Y = beta_0 + beta_1*X1 + error
length(error)
length(Y)
## Then we use this sample to estimate the least square line.
## If we were to take a different sample of Y, we would get a different least square line
## and different estimates for beta0 and beta1. Re-run lines 42 - 53 many times to see that the
## estimate coefficients would change.
lm(Y~X1)
## We hope that over many many many samples, on average, our estimates
## would exactly equal the truth.
## This is the idea of an unbiased estimator.
## How can we check this using simulations?
########################
## In-class Activity  ##
########################
## 1. Design a simulation to check whether or not our least square estimates for beta_0 and beta_1 are unbiased.
## 2. After finishing HW 1, explain why using many simple linear regression models is not sufficient
## compared to a multiple linear regression model.
## Note that fitting a simple linear regression model is computationally
## instantaneous since there are analytical solutions for our least square estimates.
## Work in groups to come up with a solution.
## Copy and paste any relevant code on Ed Discussion.
## Please be sure to list all your group members names. Only one group member needs to post on Ed Discussion.
## link: https://edstem.org/us/courses/50620/discussion/
n = 100
beta_0 = 7
beta_1 = 12
X1 = seq(0, 10, length.out = n)
B = 5000
b0 = b1 = rep(NA, B)
for (i in 1:B)
{
error = rnorm(n, 0, 1)
Y = beta_0 + beta_1*X1 + error
m1 = lm(Y ~ X1)
b0[i] = m1$coefficients[[1]]
b1[i] = m1$coefficients[[2]]
}
mean(b0) - beta_0
mean(b1) - beta_1
m1
b0
x = matrix(NA,1000,150)
for(i in 1:150){
x[,i] = rnorm(1000)
}
y = rnorm(1000)
data = as.data.frame(cbind(y,x))
head(data)
fit = lm(y~.,data=data)
summary(fit)
p_values = summary(fit)$coefficients[,4]
length(which(p_values<0.05))
beta0 = 2
beta1 = 3
beta2 = 5
beta3 = 4
beta4 = 2
beta5 = 7
error = rnorm(1000, 0, 1)
y = beta0 + beta1*x[,1] + beta2*x[,2] + beta3*x[,3] + beta4*x[,4] + beta5*x[,5] + error
data = as.data.frame(cbind(y,x))
head(data)
fit = lm(y~.,data=data)
summary(fit)
p_values = summary(fit)$coefficients[,4]
length(which(p_values<0.05))
df
df = n - (p+1)
library(ISLR2)
fit = lm(medv~crim+lstat, data=Boston)
summary(fit)
names(fit)
## hypothesis test:
#H0: beta1 = 3 vs. H1: beta1 /neq 3
#ts =
ts =  -0.07045/0.03602
#null distribution = t distribution with 503 df
n = dim(Boston)[1]
p = 2
df = n - (p+1)
#p-value: 0.0511
#conclusion: Set alpha = 0.05.
#Do not reject H0. There is not evidence
#that B1 is significantly difference from 0,
#at alpha level 0.05.
pt(abs(ts),df,lower.tail=FALSE)*2 # two-sided
pt(ts,df,lower.tail=FALSE) # one-sided
# Confidence intervals for beta0, beta1, beta2
confint(fit,level=0.99)
# Confidence intervals for E(Y) = f(x)
predict(fit,data.frame(lstat=5,crim=0.5),interval='confidence',level=0.95)
# Prediction intervals for Y
predict(fit,data.frame(lstat=5,crim=0.5),interval='prediction',level = 0.95)
# estimate of sigma^2
# sigma^2 represents the variance of Y
p = 2
sum(fit$residual^2)/(n-(p+1))
n = dim(Boston)[1]
p = 2
df = n - (p+1)
n = dim(Boston)[1]
p = 2
df = n - (p+1)
df
#### Multiple Linear Regression ####
# read in data using table read.table()
# make sure you specify the pathway where you saved the data set
patient = read.table("patient.txt",header=FALSE)
head(patient) #look at the first few rows of the data, make sure it's been loaded into R correctly
# give each column its variable name
names(patient) = c("satisf","age","severe","anxiety")
head(patient)
str(patient)
pairs(patient) #pairwise scatterplot
# linear regression model
# use the lm function
model=lm(satisf~age+severe+anxiety,data=patient)
#shortcut:
lm(satisf~.,data=patient)
### What are our least square coefficients?
### What is our fitted model?
summary(model)
names(model)
model$coefficients
model$residuals
model$fitted.values
#use this model to predict Y for specific values of X
x = data.frame(age=40,severe=40,anxiety=1.5)
predict(model,newdata=x)
########################
## In-class Activity  ##
########################
# 1. Fit a linear regression model with the response as patient satisfaction
# and use all other variables as predictors. Report the least square regression coefficients.
# 2. Based on this model, what are the predicted patient satisfaction scores
# for observations 1, 3, and 20?
# 3. Based on this model, what is the predicted patient satisfication score for a patient of age 50, disease severity
# of 27 and anxiety of 3?
# 4. Propose a way to quantify how well our model is able to predict patient
# satisfaction (Y).
## Work in groups to come up with a solution.
## Copy and paste any relevant code on Ed Discussion.
## Please be sure to list all your group members names. Only one group member needs to post on Ed Discussion.
## link: https://edstem.org/us/courses/50620/discussion/
mlr_model = lm(satisf~.,data=patient)
print(mlr_model$coefficients)
predict(model)
temp = data.frame(age=50,severe=27,anxiety=3)
predict(model, newdata = temp)
############################
##### Subset Selection #####
############################
#install.packages('leaps')
library(leaps)
library(ISLR2)
head(Hitters)
dim(Hitters)
is.na(Hitters$Salary)
sum(is.na(Hitters$Salary))
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
regfit = regsubsets(Salary~.,data=Hitters,nbest=1,nvmax=19)
regfit.sum = summary(regfit)
regfit.sum
names(regfit.sum)
n = dim(Hitters)[1]
p = rowSums(regfit.sum$which)
adjr2 = regfit.sum$adjr2
cp = regfit.sum$cp
rss = regfit.sum$rss
AIC = n*log(rss/n) + 2*(p)
BIC = n*log(rss/n) + (p)*log(n)
cbind(p,rss,adjr2,cp,AIC,BIC)
plot(p,BIC)
plot(p,AIC)
which.min(BIC)
which.min(AIC)
which.min(cp)
which.max(adjr2)
coef(regfit,6)
coef(regfit,10)
coef(regfit,11)
library(ISLR2)
Hitters = na.omit(Hitters)
library(leaps)
# we can also use regsubsets() to perform backward/forward stepwise selection
regfit.fwd = regsubsets(Salary~.,data=Hitters,nvmax=19, nbest = 1, method="forward")
regfit.bwd = regsubsets(Salary~.,data=Hitters,nvmax=19, method="backward")
regfit.fwd.sum = summary(regfit.fwd)
regfit.fwd.sum
## Forward and Backward Stepwise Selection
library(ISLR2)
Hitters = na.omit(Hitters)
library(leaps)
# we can also use regsubsets() to perform backward/forward stepwise selection
regfit.fwd = regsubsets(Salary~.,data=Hitters,nvmax=19, nbest = 1, method="forward")
regfit.bwd = regsubsets(Salary~.,data=Hitters,nvmax=19, method="backward")
regfit.fwd.sum = summary(regfit.fwd)
names(regfit.fwd.sum)
n = dim(Hitters)[1]
p = rowSums(regfit.fwd.sum$which) #number of predictors + intercept in the model
adjr2 = regfit.fwd.sum$adjr2
cp = regfit.fwd.sum$cp
rss = regfit.fwd.sum$rss
AIC = n*log(rss/n) + 2*(p)
BIC = n*log(rss/n) + (p)*log(n)
plot(AIC,type='b')
plot(BIC,type='b')
plot(cp,type='b')
plot(adjr2,type='b')
p
m1 = lm(mpg~horsepower,data=Auto)
library(ISLR2)
head(Auto)
m1 = lm(mpg~horsepower,data=Auto)
summary(m1)
par(mfrow=c(2,2))
plot(m1)
par(mfrow=c(1,2))
plot(m1,which=3)
plot(m2, which=3)
bc = boxcox(m1)
library(MASS)
bc = boxcox(m1)
names(bc)
insurance=read.csv("./insurance.csv")
str(insurance)
insurance$gender <- factor(insurance$gender)
insurance$smoker <- factor(insurance$smoker)
insurance$region <- factor(insurance$region)
insurance$gender <- relevel(insurance$gender, ref = "female")
summary(lm(charges~age+bmi+gender, data = insurance))
# Male: charges = -6986.82 + 243.19(age) + 327.54(bmi) + 1344.46
# Female: charges = -6986.82 + 243.19(age) + 327.54(bmi)
male_data <- subset(insurance, gender == "male")
female_data <- subset(insurance, gender == "female")
fit_males <- lm(charges ~ age + bmi, data = male_data)
summary(fit_males)
# charges = -8012.79 + 238.63(age) + 406.87(bmi)
fit_females <- lm(charges ~ age + bmi, data = female_data)
summary(fit_females)
# charges = -4515.22 + 246.92(age) + 241.32(bmi)
# They are not equivalent.
# Including gender as a dummy variable in the model allows for estimating
# the overall effect of gender on charges while controlling for other predictors.
# It considers the possibility that gender influences healthcare charges differently
# across age and BMI groups.
# On the other hand, fitting separate models for males
# and females treats gender as a categorical predictor with no interaction effect.
# It assumes that the relationships between age, BMI, and charges are the same
# for both genders, but with potentially different intercepts and coefficients.
