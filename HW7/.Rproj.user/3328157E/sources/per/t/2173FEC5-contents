---
title: "HW7"
author: "Gautham Suresh"
date: "2024-03-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ISLR2)
library(leaps)
library(car)
library(caret)
library(tidyverse)
```

```{r}
head(Boston)
```

### Problem #1: Multiple Linear Regression

1a. **Final model**

```{r}
regfit <- regsubsets(medv ~ ., data = Boston, nbest=1, nvmax = 12)

regfit_sum <- summary(regfit)

n = dim(Boston)[1]
p = rowSums(regfit_sum$which)

adjr2 = regfit_sum$adjr2
cp = regfit_sum$cp
rss = regfit_sum$rss
AIC = n*log(rss/n) + 2*(p)
BIC = n*log(rss/n) + (p)*log(n)

subset_table <- data.frame(RSS = rss, Adj_R2 = adjr2, Cp = cp, AIC = AIC, BIC = BIC)

predictors_smallest_AIC <- names(coef(regfit, id = which.min(subset_table$AIC)))
predictors_smallest_BIC <- names(coef(regfit, id = which.min(subset_table$BIC)))
predictors_largest_Adj_R2 <- names(coef(regfit, id = which.max(subset_table$Adj_R2)))
predictors_smallest_Cp <- names(coef(regfit, id = which.min(subset_table$Cp)))

potential_model_predictors <- predictors_smallest_BIC[!predictors_smallest_BIC %in% "(Intercept)"]

potential_formula <- as.formula(paste("medv ~", paste(best_model_predictors, collapse = " + ")))

temp_model <- lm(potential_formula, data = Boston)

```

```{r}
best_model_predictors <- predictors_smallest_BIC[!predictors_smallest_BIC %in% c("(Intercept)", "tax")]

formula <- as.formula(paste("medv ~", paste(best_model_predictors, collapse = " + ")))

# This is the final model
final_model <- lm(formula, data = Boston)

coefficients <- coef(final_model)
predictor_names <- names(coefficients)[-1]

final_model_table <- data.frame(Coefficient = coefficients[-1])

best_model_predictors <- names(coef(best_train, id = best_model_index))


print(final_model_table)

```


-   We found the final model to include the predictors: crim, zn, chas, nox, rm, dis, rad, ptratio, lstat

```{r}

n = dim(Boston)[1]

set.seed(10)
train_index = sample(1:n,n/2,rep=FALSE)

train = Boston[train_index,]
test = Boston[-train_index,]


best_train = regsubsets(medv~.,data=train,nbest=1,nvmax=12)

val_errors = rep(NA,8)
for(i in 1:8){
  test_mat = model.matrix(medv~.,data=test)
  
  coef_m = coef(best_train,id=i)

  pred = test_mat[,names(coef_m)]%*%coef_m
  val_errors[i] = mean((test$medv-pred)^2)
}

best_model_index <- which.min(val_errors)

best_model_predictors <- names(coef(best_train, id = best_model_index))

best_model_predictors <- best_model_predictors[!best_model_predictors %in% "(Intercept)"]

formula <- as.formula(paste("medv ~", paste(best_model_predictors, collapse = " + ")))

potential_final_model <- lm(formula, data = Boston)

```

1b. **Final model "workflow"**

-   We used best subset selection which performs an exhaustive search to identify the optimal combination of predictors. This method was chosen since there are a relatively small number of predictors, meaning it can find the best model in a reasonable amount of time. In this case multiple criteria were tested to determine whether the would produce different models. In most cases, they would produce different results, but in this case, all of the criteria produced the same model.

-   The validation set approach was also tested. This produced a different model than the other criteria. The model generated by the above criteria was still chosen, since the validation set approach has the drawback of randomly choosing the training and test data

```{r}
plot(final_model, which = 1)

```

1c. **Assumptions**

-   When deriving this final model, we assumed that the relationship between the response variable and the predictors are approximately linear. We also assume that the expected value of the error term $\epsilon$ is 0, the variance of $\epsilon$ is constant across all of the predictors, and the errors $e_i$ are independent of each other

-   We check if our assumption of error terms having constant variance holds by creating a residual plot. Since the plot shows no discernible pattern, our assumption holds

-   We also check if our assumption of linearity holds. This can be derived by interpreting the residual plot. The residual plot does not indicate any non-linear associations in the data, so our assumption holds

-   Based on the diagnostics, no modifications need to be made to our final model

```{r}
vif_values <- vif(temp_model)

modified_vif_values <- vif(final_model)
```

1d. **Potential issues**

-   We explore the potential issue of multicollinearity. By calculating the VIF (variance inflation factor) of our model, we can identify potential predictors that are correlated with each other. This method leads us to identify the predictors "rad" and "tax" as being highly correlated with each other as the have VIF values \> 4

-   To address this issue, we remove the predictor "tax" to reduce multicollinearity

-   The VIF values were calculated before and after removing the predictor by storing a temporary model that included "tax"

```{r}
num_folds <- 10

flds <- createFolds(Boston$medv, k = num_folds, list = TRUE, returnTrain = FALSE)

MSE_values <- numeric(num_folds)

for(i in 1:num_folds) {
  test_index <- flds[[i]]
  
  test <- Boston[test_index, ]
  train <- Boston[-test_index, ]
  
  pred_final_model <- predict(final_model, newdata = test)
  
  MSE_values[i] <- mean((test$medv - pred_final_model)^2)
}

bias <- mean(MSE_values)
variance <- var(MSE_values)
```

1e. **Bias and Variance**

-   We use cross validation with 10-folds. Based on the average of these results, we will calculate the bias and variance of the model

-   Bias = 22.98

-   Variance = 77.78

### Problem 2: Follow-up to in-class activity

```{r}
insurance <- read.csv("./insurance.csv")

insurance$gender <- factor(insurance$gender)
insurance$smoker <- factor(insurance$smoker)
insurance$region <- factor(insurance$region)
```


2a. **Models in Part 2 and Part 3**

-   The models in Part 2 and Part 3 are not equivalent because they handle the influence of gender differently
    -   In Part 2, a single model is fitted including gender as a predictor. This allows for estimating the effect of gender on healthcare charges while accounting for other predictors like age and BMI. The model accounts for potential interactions between gender and other predictors
    -   In Part 3, separate models are fitted for males and females. This approach does not consider any interaction between gender and other predictors. It assumes that the relationships between age, BMI, and charges are the same for both genders, which might not be the case

```{r}
original_model <- lm(charges ~ age + bmi, data = insurance)

# Male: charges = -6986.82 + 243.19(age) + 327.54(bmi) + 1344.46
# Female: charges = -6986.82 + 243.19(age) + 327.54(bmi)

male_data <- subset(insurance, gender == "male")
fit_males <- lm(charges ~ age + bmi, data = male_data)
# charges = -8012.79 + 238.63(age) + 406.87(bmi)

female_data <- subset(insurance, gender == "female")
fit_females <- lm(charges ~ age + bmi, data = female_data)
# charges = -4515.22 + 246.92(age) + 241.32(bmi)

```

2b.

2c.


### Problem 3: Predictions in the presence of multicollinearity

3a. Multicollinearity is a problem for making accurate predictions. With multicollinearity, it's challenging to gauge the true relationship between each independent variable and the dependent variable

```{r}
set.seed(42)

x1 <- runif(100)
x2 <- 0.8 * x1 + rnorm(100, 0, 0.1)

beta0 <- 3
beta1 <- 2
beta2 <- 4

error <- rnorm(100, mean = 0, sd = 2)

Y <- beta0 + beta1 * x1 + beta2 * x2 + error

correlation <- cor(x1, x2)
```
3b. The correlation between $x_1$ and $x_2$ is 0.94

```{r}
train_indices <- sample(1:100, 80)
train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], Y = Y[train_indices])
test_data <- data.frame(x1 = x1[-train_indices], x2 = x2[-train_indices], Y = Y[-train_indices])

model <- lm(Y ~ x1 + x2, data = train_data)

test_predictions <- predict(model, newdata = test_data)

test_mse <- mean((test_data$Y - test_predictions)^2)

```

3c. Test MSE = 3.63

```{r}
test_mses <- numeric(2500)

for (i in 1:2500) {
  epsilon <- rnorm(100, mean = 0, sd = 2)
  Y <- beta0 + beta1 * x1 + beta2 * x2 + epsilon
  
  train_indices <- sample(1:100, 80)
  train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], Y = Y[train_indices])
  test_data <- data.frame(x1 = x1[-train_indices], x2 = x2[-train_indices], Y = Y[-train_indices])
  
  model <- lm(Y ~ x1 + x2, data = train_data)
  
  test_predictions <- predict(model, newdata = test_data)
  
  test_mse <- mean((test_data$Y - test_predictions)^2)
  
  test_mses[i] <- test_mse
}

mean_test_mse <- mean(test_mses)

hist(test_mses, main = "Histogram of Test MSEs", xlab = "Test MSE")
```

3d. We find that the mean test MSE is 4.15. We see that the histogram is slightly right-skewed. A majority of the test MSE values lies around the average test MSE value as expected

```{r}
set.seed(24)

x1 <- runif(100)
x2 <- rnorm(100, 0, 1)

beta0 <- 3
beta1 <- 2
beta2 <- 4

error <- rnorm(100, mean = 0, sd = 2)

Y <- beta0 + beta1 * x1 + beta2 * x2 + error

correlation <- cor(x1, x2)
```

3e. The correlation between $x_1$ and $x_2$ is 0.033

```{r}
test_mses_uncorr <- numeric(2500)

for (i in 1:2500) {
  epsilon <- rnorm(100, mean = 0, sd = 2)
  Y <- beta0 + beta1 * x1 + beta2 * x2
  
  train_indices <- sample(1:100, 80)
  train_data <- data.frame(x1 = x1[train_indices], x2 = x2[train_indices], Y = Y[train_indices])
  test_data <- data.frame(x1 = x1[-train_indices], x2 = x2[-train_indices], Y = Y[-train_indices])
  
  model <- lm(Y ~ x1 + x2, data = train_data)
  
  test_predictions <- predict(model, newdata = test_data)
  
  test_mse <- mean((test_data$Y - test_predictions)^2)
  
  test_mses_uncorr[i] <- test_mse
}

mean_test_mse_uncorr <- mean(test_mses_uncorr)

hist(test_mses_uncorr, main = "Histogram of Test MSEs (Uncorrelated Predictors)", xlab = "Test MSE")
```

3f. When the predictors are not correlated, the average test MSE is $4.88*10^{-30}$

3g.


